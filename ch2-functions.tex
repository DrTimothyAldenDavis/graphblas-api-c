% This is ch2-functions.tex of the GraphBLAS specification.
% This is an included file. See the master file for more information.
%

%

\chapter{Mathematics}
\index{mathematics}
\label{chap:mathematics}
\section{Introduction: Graphs as Matrices}
This chapter describes the mathematics in the GraphBLAS standard.  The GraphBLAS define a narrow set of mathematical operations that have been found to be useful for implementing a wide range of graph operations.  At the heart of the GraphBLAS are 2D mathematical objects called matrices.  The matrices are usually sparse, which implies that the majority of the elements in the matrix are zero and are often not stored to make their implementation more efficient.  Sparsity is independent of the GraphBLAS mathematics.  All the mathematics defined in the GraphBLAS will work regardless of whether the underlying matrix is sparse or dense.

Graphs represent connections between vertices with edges.  Matrices can represent a wide range of graphs using \emph{adjacency} matrices or \emph{incidence} matrices.  Adjacency matrices are often easier to analyze while incidence matrices are often better for representing data.  Fortunately, the two are easily connected by the fundamental mathematical operation of the GraphBLAS: matrix-matrix multiply.  One of the great features of the GraphBLAS mathematics is that no matter what kind of graph or matrix is being used, the core operations remain the same.  In other words, a very small number of matrix operations can be used to manipulate a very wide range of graphs.

The mathematics of the GraphBLAS will be described using a ``center outward'' approach.  Initially, the most important specific cases will be described that are at the center of GraphBLAS.  The conditions on these cases will then be relaxed to arrive at more general definition.  This approach has the advantage of being more easily understandable and describing the most important cases first.

\subsection{Adjacency Matrix: Undirected Graphs, Directed Graphs, Weighted Graphs}
Given an adjacency matrix $\mathbf{A}$, if $\mathbf{A}(v_1,v_2) = 1$, then there exists an edge going from vertex $v_1$ to vertex $v_2$.  Likewise, if $\mathbf{A}(v_1,v_2) = 0$, then there is no edge from $v_1$ to $v_2$.  Adjacency matrices have direction, which means that $\mathbf{A}(v_1,v_2)$ is not the same as $\mathbf{A}(v_2,v_1)$.  Adjacency matrices can also have edge weights.  If $\mathbf{A}(v_1,v_2) = w_{12}$, and $w_{12} \neq 0$, then the edge going from $v_1$ to $v_2$ is said to have weight $w_{12}$.  Adjacency matrices provide a simple way to represent the connections between vertices in a graph between one set of vertices and another.  Adjacency matrices are often square and both out-vertices (rows) and the in-vertices (columns) are the same set of vertices.  Adjacency matrices can be rectangular in which case the out-vertices (rows) and the in-vertices (columns) are different sets of vertices.  Such graphs are often called bipartite graphs.  In summary, adjacency matrices can represent a wide range of graphs, which include any graph with any set of the following properties: directed, weighted, and/or bipartite.

\subsection{Incidence Matrix: Multi-Graphs, Hyper-Graphs, Multipartite Graphs}
An incidence, or edge matrix $\mathbf{E}$, uses the rows to represent every edge in the graph and the columns represent every vertex.  There are a number of conventions for denoting an edge in an incidence matrix.  One such convention is to set $\mathbf{E}(i,v_1) = -1$ and $\mathbf{E}(i,v_2) = 1$ to indicate that edge $i$ is a connection from $v_1$ to $v_2$.  Incidence matrices are useful because they can easily represent multi-graphs, hyper-graphs, and multi-partite graphs.  These complex graphs are difficult to capture with an adjacency matrix.  A multi-graph has multiple edges between the same vertices.  If there was another edge, $j$, from $v_1$ to $v_2$, this can be captured in an incidence matrix by setting $\mathbf{E}(j,v_1) = -1$ and $\mathbf{E}(j,v_2) = 1$.  In a hyper-graph, one edge can go between more than two vertices.  For example, to denote edge $i$ has a connection from $v_1$ to $v_2$ and $v_3$ can be accomplished by also setting $\mathbf{E}(i,v_3) = 1$.  Furthermore, $v_1$, $v_2$, and $v_3$ can be drawn from  different classes of vertices and so $\mathbf{E}$ can be used to represent multi-partite graphs.  Thus, an incidence matrix can be used to represent a graph with any set of the following graph properties: directed, weighted, multi-partite, multi-edge, and/or hyper-edge.

\section{Matrix Definition: Starting Vertices, Ending Vertices, Edge Weight Types}
  The canonical matrix of the GraphBLAS has $N$ rows and $M$ columns of real numbers.  Such a matrix can be denoted as
$$
  \mathbf{A}: \mathbb{R}^{N \times M}
$$
The canonical row and and column indexes of the matrix $\mathbf{A}$ are $i \in I = \{1,\ldots,N\}$ and $j \in J = \{1,\ldots,M\}$, so that any particular value $\mathbf{A}$ can be denoted as $\mathbf{A}(i,j)$.  [Note: a specific GraphBLAS \emph{implementation} might use IEEE 64 bit double precision floating point numbers to represent real numbers, 64 bit unsigned integers to represent row and column indices, and the compressed sparse rows (CSR) format or the compressed sparse columns (CSC) format to store the non-zero values inside the matrix.]

  A matrix of complex numbers is denoted
$$
  \mathbf{A}: \mathbb{C}^{N \times M}
$$
  A matrix of integers $\{\ldots, -1, 0, 1, \ldots\}$ is denoted
$$
  \mathbf{A}: \mathbb{Z}^{N \times M}
$$
  A matrix of natural numbers $\{1, 2, 3, \ldots\}$ is denoted
$$
  \mathbf{A}: \mathbb{N}^{N \times M}
$$

  Canonical row and column indices are natural numbers $I,J : \mathbb{N}$.  In some GraphBLAS implementations these indices could be non-negative integers  $I = \{0,\ldots,N-1\}$ and $J = \{0,\ldots,M-1\}$.
  
  For the GraphBLAS a matrix is defined as the following 2D mapping
$$
  \mathbf{A} : I \times J \rightarrow \mathbb{S}
$$
where the indices $I, J : \mathbb{Z}$ are finite sets of integers with $N$ and $M$ elements respectively, and  $\mathbb{S} \in \{\mathbb{R},\mathbb{Z},\mathbb{N}, \ldots \}$ is a set of scalars.  Without loss of generality matrices can be denoted
$$
  \mathbf{A}: \mathbb{S}^{N \times M}
$$

If the internal storage format of the matrix needs to be indicated, this can be done by
$$
  \mathbf{A}: \mathbb{S}^{N \times M}_{\rm CSC}  ~~~~~~~~ {\rm or}  ~~~~~~~~ 
  \mathbf{A}: \mathbb{S}^{N \times M}_{\rm CSR}
$$

  A \emph{vector} is a matrix where either $N=1$ or $M=1$. A column vector is denoted
$$
 \mathbf{v} = \mathbb{S}^{N \times 1}
$$
  A row vector is denoted
$$
  \mathbf{v} = \mathbb{S}^{1 \times M}
$$

  A scalar is a single element of a set $s \in \mathbb{S}$ and has no matrix dimensions.
%However, in certain implementation contexts it can sometimes be convenient to allow a single element vector/matrix $s : \mathbb{S}^{1 \times 1}$ to have scalar properties.

\section{Scalar Operations: Combining and Scaling Graph Edge Weights}
  The GraphBLAS matrix operations are built on top of scalar operations.  The primary scalar operations are standard arithmetic addition (e.g., $1 + 1 = 2$) and multiplication (e.g., $2 \times 2 = 4$).  The GraphBLAS also allow these scalar operations of addition and multiplication to be defined by the implementation or the user.  To prevent confusion with standard addition and multiplication, $\oplus$ will be used to denote scalar addition and $\otimes$ will be used to denote scalar multiplication.  In this notation, standard arithmetic addition and arithmetic multiplication of real numbers $a, b, c \in \mathbb{R}$, where $\oplus \equiv +$ and $\otimes \equiv \times$ results in
$$
   c = a \oplus b  ~~~~~~~~~ \Rightarrow ~~~~~~~~~ c = a + b
$$
and
$$
   c = a \otimes b  ~~~~~~~~~ \Rightarrow ~~~~~~~~~ c = a \times b
$$
Allowing $\oplus$ and $\otimes$ to be implementation (or user) defined functions enables the GraphBLAS to succinctly implement a wide range of algorithms on scalars of all different types (not just real numbers).

\section{Scalar Properties: Composable Graph Edge Weight Operations}
  Certain $\oplus$ and $\otimes$ combinations  over certain sets of scalars are particular useful because they preserve desirable mathematical properties such as
 %commutativity
%$$
% a \oplus b = b \oplus a ~~~~~~~~~ ~~~~~~~~~ a \otimes b = b \otimes a
%$$
associativity
$$
 (a \oplus b) \oplus c = a \oplus (b \oplus c) ~~~~~~~~~ ~~~~~~~~~ (a \otimes b) \otimes c = a \otimes (b \otimes c)
$$
and distributivity
$$
 a \otimes (b \oplus c)  = (a \otimes b) \oplus (a \otimes c)
$$

%  In most cases, if these properties are true for scalar operations they will also be true for their corresponding matrix operations.  
Associativity, and distributivity are \emph{extremely} useful properties for building graph applications because they allow the builder to swap operations without changing the result. They also increase opportunities for exploiting parallelism by the runtime. 

  Example combinations of $\oplus$ and $\otimes$ that preserve scalar associativity and distributivity include (but are not limited to) standard arithmetic
$$
  \oplus \equiv + ~~~~~~~~~ \otimes \equiv \times ~~~~~~~~~ a, b, c \in \mathbb{R}
$$
max-plus algebras
$$
  \oplus \equiv \max ~~~~~~~~~ \otimes \equiv + ~~~~~~~~~ a, b, c \in \{-\infty \cup \mathbb{R}\}
$$
min-max algebras
$$
  \oplus \equiv \max ~~~~~~~~~ \otimes \equiv \min ~~~~~~~~~ a, b, c \in [0,\infty]
$$
finite (Galois) fields such as GF(2)
$$
  \oplus \equiv {\rm xor} ~~~~~~~~~ \otimes \equiv {\rm and} ~~~~~~~~~ a, b, c \in [0,1]
$$
and power set algebras
$$
  \oplus \equiv \cup ~~~~~~~~~ \otimes \equiv \cap ~~~~~~~~~ a, b, c \subset \mathbb{Z}
$$

These operations also preserve scalar commutativity. Other functions can also be defined for $\oplus$ and $\otimes$ that do not preserve the above properties.  For example, it is often useful for $\oplus$ or $\otimes$ to pull in other data such as vertex labels of a graph, such as the select2nd operation used in breadth-first search. 

\section{Matrix Properties: Composable Operations on Entire Graphs}
  Associativity, distributivity, and commutativity are very powerful properties of the GraphBLAS and separate it from standard graph libraries because these properties allow the GraphBLAS to be composable (i.e., you can re-order operations and know that you will get the same answer).  Composability is what allows the GraphBLAS to implement a wide range of graph algorithms with just a few functions.

  Let $\mathbf{A}, \mathbf{B}, \mathbf{C} \in \mathbb{S}^{N \times M}$, be matrices with elements $a = \mathbf{A}(i,j)$, $b = \mathbf{B}(i,j)$, and $c = \mathbf{C}(i,j)$.  Associativity, distributivity, and commutativity of scalar operations translates into similar properties on matrix operations in the following manner.

\begin{description}
\item[Additive Commutativity] Allows graphs to be swapped and combined via matrix element-wise addition without changing the result
  $$
      a \oplus b = b \oplus a  ~~~~~~~~~ \Rightarrow ~~~~~~~~~
      \mathbf{A} \oplus \mathbf{B} = \mathbf{B} \oplus \mathbf{A}
  $$
  where matrix element-wise addition is given by
     $\mathbf{C}(i,j) = \mathbf{A}(i,j) \oplus \mathbf{B}(i,j)$
     
\item[Multiplicative Commutativity] Allows graphs to be swapped, intersected, and scaled via matrix element-wise multiplication without changing the result
  $$
      a \otimes b = b \otimes a  ~~~~~~~~~ \Rightarrow ~~~~~~~~~
      \mathbf{A} \otimes \mathbf{B} = \mathbf{B} \otimes \mathbf{A}
  $$
    where matrix element-wise (Hadamard) multiplication is given by
     $\mathbf{C}(i,j) = \mathbf{A}(i,j) \otimes \mathbf{B}(i,j)$

\item[Additive Associativity] Allows graphs to be combined via matrix element-wise addition in any grouping without changing the result
  $$
      (a \oplus b) \oplus c = a \oplus (b \oplus c)   ~~~~~~~~~ \Rightarrow ~~~~~~~~~
      (\mathbf{A} \oplus \mathbf{B}) \oplus \mathbf{C} = \mathbf{A} \oplus (\mathbf{B} \oplus \mathbf{C})
  $$

\item[Multiplicative Associativity] Allows graphs to be intersected and scaled via matrix element-wise multiplication in any grouping without changing the result
  $$
      (a \otimes b) \otimes c = a \otimes (b \otimes c)   ~~~~~~~~~ \Rightarrow ~~~~~~~~~
      (\mathbf{A} \otimes \mathbf{B}) \otimes \mathbf{C} = \mathbf{A} \otimes (\mathbf{B} \otimes \mathbf{C})
  $$

\item[Element-Wise Distributivity] Allows graphs to be intersected and/or scaled and then combined or vice-verse without changing the result
  $$
      a \otimes (b \oplus c) = (a \otimes b) \oplus (a \otimes c)   ~~~~~~~~~ \Rightarrow ~~~~~~~~~
      \mathbf{A} \otimes (\mathbf{B} \oplus \mathbf{C}) = (\mathbf{A} \otimes \mathbf{B}) \oplus (\mathbf{A} \otimes \mathbf{C})
  $$

\item[Matrix Multiply Distributivity] Allows graphs to be transformed via matrix multiply and then combined or vice-verse without changing the result
  $$
      a \otimes (b \oplus c) = (a \otimes b) \oplus (a \otimes c)   ~~~~~~~~~ \Rightarrow ~~~~~~~~~
      \mathbf{A} (\mathbf{B} \oplus \mathbf{C}) = (\mathbf{A} \mathbf{B}) \oplus (\mathbf{A} \mathbf{C})
  $$
  where matrix multiply $\mathbf{C} = \mathbf{A} \mathbf{B}$ is given by
  $$
   {\bf C}(i,j) = \bigoplus_{k=1}^M {\bf A}(i,k) \otimes {\bf B}(k,j)
  $$
  for matrices  ${\bf A}: \mathbb{S}^{N \times M}$,  ${\bf B}: \mathbb{S}^{M \times L}$, and ${\bf C}: \mathbb{S}^{N \times L}$

\item[Matrix Multiply Associativity] is another implication of scalar distributivity and allows graphs to be transformed via matrix multiply in any grouping without changing the result
  $$
      a \otimes (b \oplus c) = (a \otimes b) \oplus (a \otimes c)   ~~~~~~~~~ \Rightarrow ~~~~~~~~~
      (\mathbf{A} \mathbf{B}) \mathbf{C} = \mathbf{B} (\mathbf{A} \mathbf{C})
  $$
\item[Matrix Multiply Commutativity] In general, $\mathbf{A} \mathbf{B} \neq \mathbf{B} \mathbf{A}$.  Some cases where $\mathbf{A} \mathbf{B} = \mathbf{B} \mathbf{A}$ include when one matrix is all zeros, one matrix is the identity matrix, both matrices are diagonal matrices, or both matrices are rotation matrices.

\end{description}


\section{0-Element: No Graph Edge}
  Sparse matrices play an important role in GraphBLAS.  Many implementations of sparse matrices reduce storage by not storing the 0 valued elements in the matrix.  In adjacency matrices, the 0 element is equivalent to no edge from the vertex represented by row to the vertex represented by the column. In incidence matrices, the 0 element is equivalent to the edge represented by row not including the vertex represented by the column.  In most cases, the 0 element is standard arithmetic 0.  The GraphBLAS also allows the 0 element to be defined by the implementation or user.  This can be particularly helpful when combined with user defined $\oplus$ and $\otimes$ operations.  Specifically, if the 0 element has certain properties with respect scalar $\oplus$ and $\otimes$, then sparsity of matrix operations can be managed efficiently.  These properties are the additive identity
$$
     a \oplus 0 = a
$$
and the multiplicative annihilator
$$
     a \otimes 0 = 0
$$
Note: the above behavior of $\oplus$ and $\otimes$ with respect to 0 is a requirement for the GraphBLAS.

  Example combinations of $\oplus$ and $\otimes$ that exhibit the additive identity and multiplicative annihilator are:

Standard arithmetic over the real numbers $a \in \mathbb{R}$, 
$\oplus \equiv +$, $\otimes \equiv \times$, $0 \equiv 0$ $\Rightarrow$ \\
additive identity: $a \oplus 0  =  a + 0 = a$ \\
multiplicative annihilator: $a \otimes 0 = a \times 0 = 0$

Max-plus algebras over $a \in \{-\infty \cup \mathbb{R}\}$, $\oplus \equiv \max$, $\otimes \equiv +$, $0 \equiv -\infty$ $\Rightarrow$ \\
additive identity: $a \oplus 0  =  \max(a,-\infty) = a$ \\
multiplicative annihilator: $a \otimes 0 = a + -\infty = -\infty$

Min-max algebras over $a \in [0,\infty]$, $\oplus \equiv \min$, $\otimes \equiv \max$, $0 \equiv \infty$ $\Rightarrow$ \\
additive identity: $a \oplus 0  =  \min(a,\infty) = a$ \\
multiplicative annihilator: $a \otimes 0 = \max(a,\infty) = \infty$

The Galois field GF(2) over $a \in [0,1]$, $\oplus \equiv {\rm xor}$, $\otimes \equiv {\rm and}$, $0 \equiv 0$ $\Rightarrow$ \\
additive identity: $a \oplus 0  = {\rm xor}(a,0) = a$ \\
multiplicative annihilator: $a \otimes 0 = {\rm and}(a,0) = 0$

Power set algebras over sets of integers $a \subset \mathbb{Z}$, $\oplus \equiv \cup$, $\otimes \equiv \cap$, $0 \equiv \emptyset$ $\Rightarrow$ \\
additive identity: $a \oplus 0  =  a \cup \emptyset = a$ \\
multiplicative annihilator: $a \otimes 0 = a \cap \emptyset = \emptyset$


\section{Matrix Graph Operations Overview}

The core of the GraphBLAS is the ability to perform a wide range of graph operations on diverse types of graphs with a small number of matrix operations:
\begin{description}
\item[Sparse] Construct a {\bf Sparse} matrix from row, column, and value triples.  Implements graph construction from a set of starting vertices, ending vertices, and edge weights.
\item[Find] {\bf Find} the row, column, and value triples corresponding to the non-zero elements in a sparse matrix.
\item[Transpose] Flips or {\bf Transpose}s the rows and the columns of a sparse matrix.  Implements reversing the direction of the graph.  Can be implemented with  {\bf Find} and {\bf Sparse}.
\item[SpGEMM] {\bf Sp}arse {\bf GE}eneral {\bf M}atrix {\bf M}ultiplication.   Implements single-source breadth first search, multi-source breadth first search, weighted breadth first search.
\item[SpRef] {\bf Sp}arse matrix index {\bf Ref}erence. Implements sub-graph selection.  Can be implemented with {\bf Sparse} and {\bf SpGEMM}.
\item[SpAsgn] {\bf Sp}arse matrix index {\bf As}si{\bf gn}.  Implements sub-graph assignment.  Can be implemented with {\bf Sparse} and {\bf SpGEMM}.
\item[SpEWiseX] {\bf Sp}arse matrix {\bf E}lement{\bf W}ise binary operation. Implements graph union and intersection along with edge weight scale and combine.
\item[Apply] {\bf Apply} unary operation to a sparse matrix.  Implements graph edge weight modification.  Can be implemented via {\bf SpEWiseX}.
\item[Reduce] {\bf Reduce} sparse matrix.  Implements vertex degree calculations.  Can be implemented via {\bf SpGEMM}.
\end{description}
The above set of functions has been shown to be useful for implementing a wide range of graph algorithms.  These functions strike a balance between providing enough functions to be useful to an application builders and while being few enough that they can be implemented effectively.  Furthermore, from an implementation perspective, there are only four functions that are truly fundamental: {\bf Sparse}, {\bf Find}, {\bf SpGEMM}, and {\bf SpEWiseX}.  The other GraphBLAS functions can be implemented from these functions four functions.


\section{Sparse: Edge List to Graph}
  The GraphBLAS may use a variety of internal formats for representing sparse matrices.  This data can often be imported as triples of vectors ${\bf i}$, ${\bf j}$, and ${\bf v}$ corresponding to the non-zero elements in the sparse matrix.  Constructing an NxM sparse matrix from triples can be denoted
$$
   \mathbf{A} = \mathbb{S}^{N \times M}({\bf i},{\bf j},{\bf v},{\textcolor{blue}{\oplus}})
$$
where ${\bf i} : I^L$, ${\bf j}: J^L$, ${\bf i}, {\bf v}: \mathbb{S}^L$, are all $L$ element vectors, and the symbols in \textcolor{blue}{blue} represent optional operations that can be specified by the user.  The optional  ${\textcolor{blue}{\oplus}}$ function defines how multiple entries with the same row and column are handled.  If ${\textcolor{blue}{\oplus}}$ is undefined then the default is to combine the values using standard arithmetic addition $+$.  Other variants include replacing any or all of the vector inputs with single element vectors.  For example
$$
   \mathbf{A} = \mathbb{S}^{N \times M}({\bf i},{\bf j},1)
$$
would use the value of $1$ for input values.  Likewise, a row vector can be constructed using
$$
   \mathbf{A} = \mathbb{S}^{N \times M}(1,{\bf j},{\bf v})
$$
and a column vector can be constructed using
$$
   \mathbf{A} = \mathbb{S}^{N \times M}({\bf i},1,{\bf v})
$$
The value type of the sparse matrix can be further specified via
$$
   \mathbf{A}: \mathbb{R}^{N \times M}({\bf i},{\bf j},{\bf v})
$$

\section{Find: Graph to Vertex List}
  It is expected the GraphBLAS will need to send results to other software components.  Triples are a common interchange format.  The GraphBLAS {\bf find} command performs this operation by extracting the non-zero triples from a sparse matrix and can be denoted mathematically as
$$
	({\bf i},{\bf j},{\bf v}) = {\bf A}
$$

\section{Transpose: Swap Start and End Vertices}
  Swapping the rows and columns of a sparse matrix is a common tool for changing the direction of vertices in a graph.  The transpose is denoted as
$$
     {\bf B} = {\bf A}^{\sf T}
$$   
or more explicitly
$$
     {\bf B}(j,i) = {\bf A}(i,j)
$$   
where ${\bf A}:\mathbb{S}^{N \times M}$ and ${\bf B}:\mathbb{S}^{M \times N}$

Transpose can be implemented using a combination of sparse and find as follows
$$
	({\bf i},{\bf j},{\bf v}) = {\bf A}
$$
$$
   \mathbf{B} = \mathbb{S}^{M \times N}({\bf j},{\bf i},{\bf v})
$$

  
\section{SpGEMM: Weighted, Multi-Source, Breadth-First-Search}

Matrix multiply is the most important operation in the GraphBLAS and can be used to implement a wide range of graph algorithms.  In its most common form, SpGEMM performs a matrix multiply using standard arithmetic addition and multiplication
$$
   {\bf C} = {\bf A} {\bf B}
$$
or more explicitly
$$
   {\bf C}(i,j) = \sum_{k=1}^M {\bf A}(i,k) {\bf B}(k,j)
$$
where ${\bf A}: \mathbb{R}^{N \times M}$,  ${\bf B}: \mathbb{R}^{M \times L}$, and ${\bf C}: \mathbb{R}^{N \times L}$. {\bf SpGEMM} has many important variants that include accumulating results, transposing inputs or outputs, and user defined addition and multiplication.  These variants can be used alone or in combination.  When these variants are combined with the wide range of graphs that can be represented with sparse matrices, this results in many thousands of distinct graph operations that can be succinctly captured by multiplying two sparse matrices.   As will be described subsequently, all of these variants can be represented by the following mathematical statement
$$
   {\bf C}^{\textcolor{blue}{\sf T}} ~~ {\textcolor{blue}{\oplus}}{=} ~~ {\bf A}^{\textcolor{blue}{\sf T}} ~ {\oplus}.{\otimes} ~ {\bf B}^{\textcolor{blue}{\sf T}}
$$
where  ${\bf A}: \mathbb{S}^{N \times M}$,  ${\bf B}: \mathbb{S}^{M \times L}$, and ${\bf C}: \mathbb{S}^{N \times L}$, ${\textcolor{blue}{\oplus}}{=}$ denotes the option of adding the product to the existing values in ${\bf C}$, and ${\oplus}.{\otimes}$ makes explicit that ${\oplus}$ and ${\otimes}$ can be user defined functions.

\subsection{Accumulation: Summing up Edge Weights}

{\bf SpGEMM} can be used to multiply and accumulate values into a matrix.  One example is when the result of multiply ${\bf A}$ and ${\bf B}$ is added to the existing values in ${\bf C}$ (instead of replacing ${\bf C}$.  This can be written
$$
   {\bf C} ~~ {+}{=} ~~ {\bf A} {\bf B}
$$
or more explicitly
$$
   {\bf C}(i,j) = {\bf C}(i,j) + \sum_{k=1}^M {\bf A}(i,k) {\bf B}(k,j)
$$

\subsection{Transposing Inputs or Outputs: Swapping Start and End Vertices}

Another variant is to specify that the matrix multiply should be performed over the transpose of  ${\bf A}$, ${\bf B}$, or ${\bf C}$.

Transposing the input matrix ${\bf A}$ implies
$$
   {\bf C} = {\bf A}^{\sf T} {\bf B}
$$
or more explicitly
$$
   {\bf C}(i,j) = \sum_{k=1}^M {\bf A}(k,i) {\bf B}(k,j)
$$
where ${\bf A}: \mathbb{R}^{M \times N}$.

Transposing the input matrix ${\bf B}$ implies
$$
   {\bf C} = {\bf A} {\bf B}^{\sf T}
$$
or more explicitly
$$
   {\bf C}(i,j) = \sum_{k=1}^M {\bf A}(i,k) {\bf B}(j,k)
$$
where ${\bf B}: \mathbb{R}^{L \times M}$.


Transposing the output matrix ${\bf C}$ implies
$$
   {\bf C}^{\sf T} = {\bf A} {\bf B}
$$
or more explicitly
$$
   {\bf C}(j,i) = \sum_{k=1}^M {\bf A}(i,k) {\bf B}(k,j)
$$
where ${\bf C}: \mathbb{R}^{L \times N}$.

Other combinations include transposing both inputs ${\bf A}$ and ${\bf B}$
$$
   {\bf C} = {\bf A}^{\sf T} {\bf B}^{\sf T} ~~~~~~~~ \Rightarrow ~~~~~~~~ {\bf C}(i,j) = \sum_{k=1}^M {\bf A}(k,i) {\bf B}(j,k)
$$
where ${\bf A}: \mathbb{R}^{M \times N}$ and ${\bf B}: \mathbb{R}^{L \times M}$; transposing both input ${\bf A}$ and output ${\bf C}$
$$
   {\bf C}^{\sf T} = {\bf A}^{\sf T} {\bf B} ~~~~~~~~ \Rightarrow ~~~~~~~~ {\bf C}(j,k) = \sum_{k=1}^M {\bf A}(k,i) {\bf B}(k,j)
$$
where ${\bf A}: \mathbb{R}^{M \times N}$ and ${\bf C}: \mathbb{R}^{L \times N}$; and transposing both input ${\bf B}$ and output ${\bf C}$
$$
   {\bf C}^{\sf T} = {\bf A} {\bf B}^{\sf T} ~~~~~~~~ \Rightarrow ~~~~~~~~ {\bf C}(j,k) = \sum_{k=1}^M {\bf A}(i,k) {\bf B}(j,k)
$$
where ${\bf B}: \mathbb{R}^{L \times M}$ and ${\bf C}: \mathbb{R}^{L \times N}$.

Normally, the transpose operation distributes over matrix multiplication $({\bf A} {\bf B})^{\sf T} ={\bf B}^{\sf T} {\bf A}^{\sf T}$ and so transposing both inputs ${\bf A}$ and ${\bf B}$ and the output ${\bf C}$ is rarely used.  Nevertheless, for completeness, this operation is defined as
$$
   {\bf C}^{\sf T} = {\bf A}^{\sf T} {\bf B}^{\sf T} ~~~~~~~~ \Rightarrow ~~~~~~~~ {\bf C}(j,i) = \sum_{k=1}^M {\bf A}(k,i) {\bf B}(j,k)
$$
where ${\bf A}: \mathbb{R}^{M \times N}$, ${\bf B}: \mathbb{R}^{L \times M}$, and ${\bf C}: \mathbb{R}^{L \times N}$.

\subsection{Addition and Multiplication: Combining and Scaling Edges}
  Standard matrix multiplication on real numbers first performs scalar arithmetic multiplication on the elements and then performs scalar arithmetic addition on the results.  The GraphBLAS allows the scalar operations of addition $\oplus$ and multiplication $\otimes$ to be replaced with user defined functions.  This can be formally denoted as
$$
   {\bf C} = {\bf A} ~ {\oplus}.{\otimes} ~ {\bf B}
$$
or more explicitly
$$
   {\bf C}(i,j) = \bigoplus_{k=1}^M {\bf A}(i,k) \otimes {\bf B}(k,j)
$$
where ${\bf A}: \mathbb{S}^{N \times M}$,  ${\bf B}: \mathbb{S}^{M \times L}$, and ${\bf C}: \mathbb{S}^{N \times L}$.  In this notation, standard matrix multiply can be written
$$
   {\bf C} = {\bf A} ~ {+}.{\times} ~ {\bf B}
$$
where $\mathbb{S} \rightarrow \mathbb{R}$. Other matrix multiplications of interest include max-plus algebras
$$
   {\bf C} = {\bf A} ~ {\max}.{+} ~ {\bf B}
$$
or more explicitly
$$
   {\bf C}(i,j) = \max_k \{{\bf A}(i,k) + {\bf B}(k,j)\}
$$
where $\mathbb{S} \rightarrow \{-\infty \cup \mathbb{R}\}$; min-max algebras
$$
   {\bf C} = {\bf A} ~ {\min}.{\max} ~ {\bf B}
$$
or more explicitly
$$
   {\bf C}(i,j) = \min_k \{\max({\bf A}(i,k),{\bf B}(k,j))\}
$$
where $\mathbb{S} \rightarrow [0,\infty)$; the Galois field of order 2
$$
   {\bf C} = {\bf A} ~ {\rm xor}.{\rm and} ~ {\bf B}
$$
or more explicitly
$$
   {\bf C}(i,j) = {\rm xor}_k \{\rm and({\bf A}(i,k),{\bf B}(k,j))\}
$$
where $\mathbb{S} \rightarrow [0,1]$; and power set algebras
$$
   {\bf C} = {\bf A} ~ {\cup}.{\cap} ~ {\bf B}
$$
or more explicitly
$$
   {\bf C}(i,j) = \bigcup_{k=1}^M {\bf A}(i,k) \cap {\bf B}(k,j)
$$
where $\mathbb{S} \rightarrow \{\mathbb{Z}\}$.

  Accumulation also works with user defined addition and can be denoted
$$
   {\bf C} ~~ {\oplus}{=} ~~ {\bf A} ~ {\oplus}.{\otimes} ~ {\bf B}
$$
or more explicitly
$$
   {\bf C}(i,j) = {\bf C}(i,j) \oplus \bigoplus_{k=1}^M {\bf A}(i,k) \otimes {\bf B}(k,j)
$$


\section{SpRef: Selecting Sub-Graphs}
  Selecting sub-graphs is a very common graph operation.  The GraphBLAS performs this operation with the {\bf SpRef} function by selecting starting vertices (row) and ending vertices (columns) from a matrix $\mathbf{A} : \mathbb{S}^{N \times M}$
$$
   \mathbf{B}= \mathbf{A}({\bf i},{\bf j})
$$
or more explicitly
$$
   {\bf B}(i,j) = {\bf A}({\bf i}(i),{\bf j}(j))
$$
where $i \in \{1,...,N_B\}$, $j \in \{1,...,M_B\}$, ${\bf i} : I^{N_B}$, and ${\bf j}: J^{M_B}$  select specific sets of rows and columns in a specific order.   The resulting matrix $\mathbf{B} : \mathbb{S}^{N_B \times M_B}$ can be larger or smaller than the input matrix $\mathbf{A}$.  {\bf SpRef} can also be used to replicate and/or permute rows and columns in a matrix.

  {\bf SpRef} can be implemented using sparse matrix multiply as
$$
   \mathbf{B}= \mathbf{S}({\bf i}) ~ \mathbf{A} ~ \mathbf{S}^{\sf T}({\bf j})
$$
where $\mathbf{S}({\bf i})$ and $\mathbf{S}({\bf j})$ are selection matrices given by
$$
   \mathbf{S}({\bf i}) = \mathbb{S}^{N_B \times N}(\{1,...,N_B\},{\bf i},1)
$$
$$
    \mathbf{S}({\bf j}) = \mathbb{S}^{M_B \times M}(\{1,...,M_B\},{\bf j},1)
$$


\section{SpAsgn: Modifying Sub-Graphs}
  Modifying sub-graphs is a very common graph operation.  The GraphBLAS performs this operation with the {\bf SpAsgn} function by selecting starting vertices (row) and ending vertices (columns) from a matrix $\mathbf{A} : \mathbb{S}^{N \times M}$ and assigning new values to them from another sparse matrix
$$
   \mathbf{A}({\bf i},{\bf j}) ~~ {\textcolor{blue}{\oplus}}{=} ~~ \mathbf{B}
$$
or more explicitly
$$
   {\bf A}({\bf i}(i),{\bf j}(j)) ~~ {\textcolor{blue}{\oplus}}{=} ~~{\bf B}(i,j) 
$$
where $i \in \{1,...,N_B\}$, $j \in \{1,...,M_B\}$, ${\bf i} : I^{N_B}$ and ${\bf j}: J^{M_B}$ select specific sets of rows and columns in a specific order and ${\textcolor{blue}{\oplus}}$ optionally allows $\mathbf{B}$ to added to the existing values of $\mathbf{A}$. 

  The additive form of {\bf SpAsgn} can be implemented using sparse matrix multiply as
$$
   \mathbf{A} ~~ {\oplus}{=} ~~ \mathbf{S}^{\sf T}({\bf i}) ~ \mathbf{B} ~ \mathbf{S}({\bf j})
$$
where $\mathbf{S}({\bf i})$ and $\mathbf{S}({\bf j})$ are selection matrices given by
$$
   \mathbf{S}({\bf i}) = \mathbb{S}^{N_B \times N}(\{1,...,N_B\},{\bf i},1)
$$
$$
    \mathbf{S}({\bf j}) = \mathbb{S}^{M_B \times M}(\{1,...,M_B\},{\bf j},1)
$$
  

\section{SpEWiseX: Combining Graphs, Intersecting Graphs, Scaling Graphs}
  Combining graphs along with adding their edge weights can be accomplished by adding together their sparse matrix representations
$$
   {\bf C} ~~ {\oplus}{=} ~~ {\bf A}
$$
or more explicitly
$$
   {\bf C}(i,j) = {\bf C}(i,j) \oplus {\bf A}(i,j)
$$
where ${\bf A},{\bf C}: \mathbb{S}^{N \times M}$, $i \in \{1,...,N\}$, and $j \in \{1,...,M\}$.

  Intersecting graphs along with scaling their edge weights can be accomplished by element-wise multiplication of their sparse matrix representations
$$
   {\bf C} = {\bf A} \otimes {\bf B}
$$
or more explicitly
$$
   {\bf C}(i,j) = {\bf A}(i,j) \otimes {\bf B}(i,j)
$$
where ${\bf A},{\bf B},{\bf C}: \mathbb{S}^{N \times M}$.

  {\bf SpEWiseX} combines both of these operations into a single function
$$
   {\bf C} ~~ {\textcolor{blue}{\oplus}}{=} ~~ {\bf A} ~~ {\textcolor{blue}{\otimes}} ~~ {\textcolor{blue}{\bf B}}
$$
or more explicitly
$$
   {\bf C}(i,j) = {\textcolor{blue}{\bf C}}(i,j) ~ {\textcolor{blue}{\oplus}} ~ {\bf A}(i,j) ~~ {\textcolor{blue}{\otimes}} ~~ {\textcolor{blue}{\bf B}}(i,j)
$$
where ${\textcolor{blue}{\oplus}}$, ${\textcolor{blue}{\otimes}}$, and ${\textcolor{blue}{\bf B}}$ are optional arguments. 

\section{Apply: Modify Edge Weights}
  Modifying edge weights can be done by via the element-wise by unary function $f()$ to the values of a sparse matrix
$$
   {\bf C} ~~ {\textcolor{blue}{\oplus}}{=} ~~ f({\bf A})
$$
or more explicitly
$$
   {\bf C}(i,j) = {\textcolor{blue}{\bf C}}(i,j) ~ {\textcolor{blue}{\oplus}} ~f({\bf A}(i,j))
$$
where ${\bf A},{\bf C}: \mathbb{S}^{N \times M}$, and $f(0) = 0$.

  {\bf Apply} can be implemented via {\bf SpEWiseX} via
$$
   {\bf C} ~~ {\textcolor{blue}{\oplus}}{=} ~~ {\bf A} \otimes {\bf A}
$$
where $\otimes \equiv f()$ and $f(a,a) = f(a)$.

\section{Reduce: Compute Vertex Degrees}
  It is often desired to combine the weights of all the vertices that come out of the same starting vertices.  This aggregation can be represented as a matrix product as
$$
   {\bf c} ~~ {\textcolor{blue}{\oplus}}{=} {\bf A} ~~ {\oplus}.{\otimes} ~~{\bf 1}
$$
or more explicitly
$$
   {\bf c}(i,1) = {\textcolor{blue}{\bf c}}(i,1) ~~ {\textcolor{blue}{\oplus}} ~~ \bigoplus_{j=1}^M {\bf A}(i,j)
$$
where ${\bf c}: \mathbb{S}^{N \times 1}$ and ${\bf A}: \mathbb{S}^{N \times M}$, and ${\bf 1}: \mathbb{S}^{M \times 1}$ is a column vector of all ones.

Likewise, combining all the weights of all the vertices that go into the same ending vertices can be represented as matrix product as
$$
   {\bf c} ~~ {\textcolor{blue}{\oplus}}{=}  {\bf 1} ~~ {\oplus}.{\otimes} ~~{\bf A} 
$$
or more explicitly
$$
   {\bf c}(1,j) = {\textcolor{blue}{\bf c}}(1,j) ~~ {\textcolor{blue}{\oplus}} ~~ \bigoplus_{i=1}^N {\bf A}(i,j)
$$
where ${\bf c}: \mathbb{S}^{1 \times M}$ and ${\bf A}: \mathbb{S}^{N \times M}$, and ${\bf 1}: \mathbb{S}^{1 \times N}$ is a row vector of all ones.


\section{Appendix: Glossary}
  [Will eventually be moved to front matter.]

\section{Appendix: Examples}
  [Will eventually be moved to back matter.]


For my own reference, I'm leaving in a few latex fragments I'd like to remember for when we add our own text.


\begin{boxedcode}
\#pragma\plc{ }omp\plc{ directive-name [clause[ [},\plc{] clause] ... ] new-line}
\end{boxedcode}

As an example of how a construct might look, here is the single construct from OpenMP

\subsection{\code{single} Construct}
\index{single@{\code{single}}}
\index{constructs!single@{\code{single}}}
\label{subsec:single Construct}
\summary
The \code{single} construct specifies that the associated structured block is executed by only 
one of the threads in the team (not necessarily the master thread), in the context of its 
implicit task. The other threads in the team, which do not execute the block, wait at an 
implicit barrier at the end of the \code{single} construct unless a \code{nowait} clause is specified.

\parbox{\linewidth}{%
\syntax
\ccppspecificstart}
The syntax of the single construct is as follows:

\begin{boxedcode}
\#pragma omp single \plc{[clause[ [},\plc{] clause] ... ] new-line}
   \plc{structured-block}
\end{boxedcode}

\begin{samepage}
where \plc{clause} is one of the following:

\begin{indentedcodelist}
private(\plc{list})
firstprivate(\plc{list})
copyprivate(\plc{list})
nowait
\end{indentedcodelist}
\ccppspecificend
\end{samepage}

\fortranspecificstart
The syntax of the \code{single} construct is as follows:

\begin{boxedcode}
!\$omp single \plc{[clause[ [},\plc{] clause] ... ]}
   \plc{structured-block} 
!\$omp end single \plc{[end\_clause[ [},\plc{] end\_clause] ... ]}
\end{boxedcode}

where \plc{clause} is one of the following:

\begin{indentedcodelist}
private(\plc{list})
firstprivate(\plc{list})
\end{indentedcodelist}

and \plc{end\_clause} is one of the following: 

\begin{indentedcodelist}
copyprivate(\plc{list})
nowait
\end{indentedcodelist}
\fortranspecificend

\binding
The binding thread set for a \code{single} region is the current team. A \code{single} region 
binds to the innermost enclosing \code{parallel} region. Only the threads of the team 
executing the binding \code{parallel} region participate in the execution of the structured 
block and the implied barrier of the \code{single} region if the barrier is not eliminated by a 
\code{nowait} clause.

\descr
The method of choosing a thread to execute the structured block is implementation 
defined. There is an implicit barrier at the end of the \code{single} construct unless a 
\code{nowait} clause is specified. 

\restrictions
Restrictions to the \code{single} construct are as follows: 

\begin{itemize}
\item The \code{copyprivate} clause must not be used with the \code{nowait} clause.

\item At most one \code{nowait} clause can appear on a \code{single} construct.

\cppspecificstart
\item A throw executed inside a \code{single} region must cause execution to resume within the 
same \code{single} region, and the same thread that threw the exception must catch it.
\cppspecificend
\end{itemize}


\crossreferences
\begin{itemize}
\item \code{private} and \code{firstprivate} clauses, see 
\specref{subsec:Data-Sharing Attribute Clauses}.

\item \code{copyprivate} clause, see 
\specref{subsubsec:copyprivate clause}.
\end{itemize}


% This is the end of ch2-functions.tex of the GraphBLAS specification.

